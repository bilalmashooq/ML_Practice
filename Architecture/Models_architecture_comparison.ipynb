{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## **Dataset Overview: CIFAR-10**\n",
    "\n",
    "Before diving into the implementations, here's a brief overview of the CIFAR-10 dataset:\n",
    "\n",
    "- **Number of Classes:** 10 (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck)\n",
    "- **Image Size:** 32x32 pixels, 3 channels (RGB)\n",
    "- **Training Samples:** 50,000\n",
    "- **Testing Samples:** 10,000\n",
    "\n",
    "The dataset is commonly used for image classification tasks and provides a standardized benchmark for evaluating different machine learning models.\n",
    "\n",
    "\n"
   ],
   "id": "e4e1ab68ce530d3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **1. TensorFlow Implementation**\n",
    "\n",
    "### **Step-by-Step Code with Detailed Comments**\n",
    "\n"
   ],
   "id": "b60d9dc75aad7831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TensorFlow Implementation for CIFAR-10 Classification\n",
    "\n",
    "# Step 1: Import Necessary Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# Step 2: Load and Preprocess the CIFAR-10 Dataset\n",
    "# Load the dataset; it returns training and testing data\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1 by dividing by 255\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Step 3: Define the CNN Model Architecture using Sequential API\n",
    "model = models.Sequential([\n",
    "    # First Convolutional Layer:\n",
    "    # - 32 filters\n",
    "    # - 3x3 kernel size\n",
    "    # - ReLU activation\n",
    "    # - Input shape matches CIFAR-10 images (32x32x3)\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    \n",
    "    # First Max Pooling Layer:\n",
    "    # - 2x2 pool size reduces spatial dimensions\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional Layer:\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Second Max Pooling Layer:\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third Convolutional Layer:\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Flatten the 3D feature maps to 1D feature vectors\n",
    "    layers.Flatten(),\n",
    "    \n",
    "    # Fully Connected Layer with 64 units and ReLU activation\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output Layer with 10 units (one for each class) and softmax activation\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',                      # Adam optimizer for training\n",
    "              loss='sparse_categorical_crossentropy',# Loss function suitable for integer labels\n",
    "              metrics=['accuracy'])                  # Metric to monitor during training\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(train_images, train_labels, \n",
    "          epochs=10,                                # Number of epochs\n",
    "          batch_size=64,                            # Number of samples per gradient update\n",
    "          validation_data=(test_images, test_labels))  # Data for validating the model\n",
    "\n",
    "# Step 6: Evaluate the Model on Test Data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")"
   ],
   "id": "4d5c4375787d5bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "167390ba6126105d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Explanation of TensorFlow Code**\n",
    "\n",
    "1. **Import Libraries:** TensorFlow and Keras modules are imported to build and train the CNN.\n",
    "\n",
    "2. **Load and Preprocess Data:** The CIFAR-10 dataset is loaded, and pixel values are normalized to improve training performance.\n",
    "\n",
    "3. **Define Model Architecture:** A Sequential model is constructed with three convolutional layers interleaved with max-pooling layers, followed by a flattening layer and two dense layers. The final dense layer uses softmax activation for multi-class classification.\n",
    "\n",
    "4. **Compile the Model:** The model is compiled with the Adam optimizer, using sparse categorical cross-entropy as the loss function since the labels are integers. Accuracy is set as the metric to monitor.\n",
    "\n",
    "5. **Train the Model:** The model is trained for 10 epochs with a batch size of 64. Validation data is provided to monitor performance on unseen data during training.\n",
    "\n",
    "6. **Evaluate the Model:** After training, the model's performance is evaluated on the test dataset, and the test accuracy is printed.\n",
    "\n",
    "---"
   ],
   "id": "84f911803b2bb87b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **2. PyTorch Implementation**\n",
    "\n",
    "### **Step-by-Step Code with Detailed Comments**"
   ],
   "id": "e078d08f73555830"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PyTorch Implementation for CIFAR-10 Classification\n",
    "\n",
    "# Step 1: Import Necessary Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Step 2: Define Transformations for Data Augmentation and Normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),  # Normalize each channel (R, G, B) to have mean=0.5\n",
    "                         (0.5, 0.5, 0.5))  # and standard deviation=0.5\n",
    "])\n",
    "\n",
    "# Step 3: Load the CIFAR-10 Training and Testing Datasets\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# Step 4: Define the CNN Model Architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # First Convolutional Layer:\n",
    "        # - 32 filters\n",
    "        # - 3x3 kernel size\n",
    "        # - Stride of 1\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride=1, padding=1)\n",
    "        \n",
    "        # Second Convolutional Layer:\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=1, padding=1)\n",
    "        \n",
    "        # Third Convolutional Layer:\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n",
    "        \n",
    "        # Max Pooling Layer with 2x2 kernel\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Fully Connected Layer with 64 units\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 64)  # CIFAR-10 images are 32x32. After three poolings, size is 4x4\n",
    "        \n",
    "        # Output Layer with 10 units (one for each class)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "        # Activation Function\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply first convolutional layer followed by ReLU activation and pooling\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        \n",
    "        # Apply second convolutional layer followed by ReLU activation and pooling\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        \n",
    "        # Apply third convolutional layer followed by ReLU activation and pooling\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        \n",
    "        # Flatten the tensor into a vector for the fully connected layers\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        \n",
    "        # Apply first fully connected layer followed by ReLU activation\n",
    "        x = self.relu(self.fc1(x))\n",
    "        \n",
    "        # Apply output layer\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Step 5: Instantiate the Model, Define Loss Function and Optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n",
    "model = CNN().to(device)  # Move model to the selected device\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Suitable loss for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "\n",
    "# Step 6: Train the Model\n",
    "num_epochs = 10  # Number of epochs to train\n",
    "\n",
    "for epoch in range(num_epochs):  # Loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the selected device\n",
    "        \n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        \n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute loss\n",
    "        loss.backward()  # Backward pass (compute gradients)\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    # Print average loss for this epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# Step 7: Evaluate the Model on Test Data\n",
    "model.eval()  # Set model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # Move data to the selected device\n",
    "        outputs = model(images)  # Forward pass\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n"
   ],
   "id": "54fdc1bd6a333384",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb82452ee8b43b73"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Explanation of PyTorch Code**\n",
    "\n",
    "1. **Import Libraries:** PyTorch and torchvision libraries are imported for building and training the CNN.\n",
    "\n",
    "2. **Data Transformations:** The images are converted to tensors and normalized. Normalization helps in speeding up the training and achieving better performance.\n",
    "\n",
    "3. **Load Datasets:** The CIFAR-10 training and testing datasets are loaded using `torchvision.datasets.CIFAR10`. Data loaders are created to handle batching and shuffling.\n",
    "\n",
    "4. **Define Model Architecture:** A custom CNN class is defined by subclassing `nn.Module`. It includes three convolutional layers with ReLU activations and max-pooling, followed by two fully connected layers.\n",
    "\n",
    "5. **Instantiate Model, Define Loss and Optimizer:** The model is moved to the GPU if available. Cross-entropy loss is used for multi-class classification, and the Adam optimizer is selected for training.\n",
    "\n",
    "6. **Train the Model:** The training loop iterates over the dataset for a specified number of epochs. For each batch, it performs a forward pass, computes the loss, performs backpropagation, and updates the weights.\n",
    "\n",
    "7. **Evaluate the Model:** After training, the model is evaluated on the test dataset. The accuracy is computed by comparing the predicted labels with the true labels.\n",
    "\n",
    "---\n"
   ],
   "id": "a67692cc1c3f068b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# **3. Keras Implementation (with TensorFlow Backend)**\n",
    " **Step-by-Step Code with Detailed Comments**\n"
   ],
   "id": "9bf7e72c97cc8ea0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Keras Implementation for CIFAR-10 Classification\n",
    "\n",
    "# Step 1: Import Necessary Libraries\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Step 2: Load and Preprocess the CIFAR-10 Dataset\n",
    "# Load the dataset; it returns training and testing data\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1 by dividing by 255\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Optional: Convert class vectors to binary class matrices (one-hot encoding)\n",
    "# Not necessary if using 'sparse_categorical_crossentropy' loss\n",
    "# train_labels = to_categorical(train_labels, 10)\n",
    "# test_labels = to_categorical(test_labels, 10)\n",
    "\n",
    "# Step 3: Define the CNN Model Architecture using Sequential API\n",
    "model = Sequential([\n",
    "    # First Convolutional Layer:\n",
    "    # - 32 filters\n",
    "    # - 3x3 kernel size\n",
    "    # - ReLU activation\n",
    "    # - Input shape matches CIFAR-10 images (32x32x3)\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    \n",
    "    # First Max Pooling Layer:\n",
    "    # - 2x2 pool size reduces spatial dimensions\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Second Convolutional Layer:\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Second Max Pooling Layer:\n",
    "    MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Third Convolutional Layer:\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    \n",
    "    # Flatten the 3D feature maps to 1D feature vectors\n",
    "    Flatten(),\n",
    "    \n",
    "    # Fully Connected Layer with 64 units and ReLU activation\n",
    "    Dense(64, activation='relu'),\n",
    "    \n",
    "    # Output Layer with 10 units (one for each class) and softmax activation\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',                      # Adam optimizer for training\n",
    "              loss='sparse_categorical_crossentropy',# Loss function suitable for integer labels\n",
    "              metrics=['accuracy'])                  # Metric to monitor during training\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(train_images, train_labels, \n",
    "          epochs=10,                                # Number of epochs\n",
    "          batch_size=64,                            # Number of samples per gradient update\n",
    "          validation_data=(test_images, test_labels))  # Data for validating the model\n",
    "\n",
    "# Step 6: Evaluate the Model on Test Data\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n"
   ],
   "id": "87065873d9d92849",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### **Explanation of Keras Code**\n",
    "\n",
    "1. **Import Libraries:** Keras modules are imported to build and train the CNN. Keras is integrated with TensorFlow as its backend.\n",
    "\n",
    "2. **Load and Preprocess Data:** The CIFAR-10 dataset is loaded, and pixel values are normalized to improve training performance. Optionally, labels can be one-hot encoded, but it's not necessary when using `sparse_categorical_crossentropy` loss.\n",
    "\n",
    "3. **Define Model Architecture:** A Sequential model is constructed with three convolutional layers interleaved with max-pooling layers, followed by a flattening layer and two dense layers. The final dense layer uses softmax activation for multi-class classification.\n",
    "\n",
    "4. **Compile the Model:** The model is compiled with the Adam optimizer, using sparse categorical cross-entropy as the loss function since the labels are integers. Accuracy is set as the metric to monitor.\n",
    "\n",
    "5. **Train the Model:** The model is trained for 10 epochs with a batch size of 64. Validation data is provided to monitor performance on unseen data during training.\n",
    "\n",
    "6. **Evaluate the Model:** After training, the model's performance is evaluated on the test dataset, and the test accuracy is printed.\n"
   ],
   "id": "2429debb517b72ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## **Summary and Comparison**\n",
    "\n",
    "| **Aspect**             | **TensorFlow**                                       | **PyTorch**                                     | **Keras (TensorFlow)**                          |\n",
    "|------------------------|------------------------------------------------------|-------------------------------------------------|-------------------------------------------------|\n",
    "| **Model Definition**   | Sequential API with Keras layers                      | Custom `nn.Module` class with explicit layers   | Sequential API with Keras layers                 |\n",
    "| **Training Process**   | High-level `model.fit()` method                        | Manual training loop with forward and backward passes | High-level `model.fit()` method                |\n",
    "| **Data Handling**      | `tf.data` pipelines and built-in data augmentation      | `torchvision.datasets` and `DataLoader`         | Keras data generators and built-in preprocessing |\n",
    "| **Flexibility**        | Highly flexible with TensorFlow's extensive features    | Extremely flexible with dynamic computation graphs | Moderate flexibility, easy to use               |\n",
    "| **Ease of Use**        | Intermediate; requires understanding of TensorFlow      | Requires more boilerplate code                  | Very user-friendly and easy to prototype         |\n",
    "| **Performance**        | Optimized for production and scalability                | Preferred for research and experimentation      | Excellent for rapid development and prototyping   |\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "- **TensorFlow** offers a balance between flexibility and ease of use, making it suitable for both research and production environments.\n",
    "  \n",
    "- **PyTorch** provides unparalleled flexibility and control, making it the go-to choice for researchers and those who require dynamic computation graphs.\n",
    "  \n",
    "- **Keras** (with TensorFlow backend) is the most user-friendly option, ideal for beginners and rapid prototyping due to its simple and intuitive API.\n",
    "\n",
    "\n"
   ],
   "id": "3729ae0a872510e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "36e30987f8be50b8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
