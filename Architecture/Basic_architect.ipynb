{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TensorFlow vs. PyTorch vs. Keras: Model Building and Training Workflow\n",
    "\n",
    "Let’s go deeper into the architecture, workflow, and implementation of models using **TensorFlow**, **PyTorch**, and **Keras**, and understand how they handle inputs, processing, and outputs. I will explain the internal architecture, from creating models, training them, and finally generating outputs, specific to these frameworks.\n",
    "\n",
    "\n",
    "\n",
    "### **1. TensorFlow**  \n",
    "TensorFlow is designed around computational graphs and offers both low-level and high-level APIs (such as Keras for easy model building).\n",
    "\n",
    "#### **Architecture Overview**:\n",
    "- **Tensors**: Multi-dimensional arrays that flow through the graph.\n",
    "- **Computational Graph**: Consists of nodes (operations) and edges (data flow). The graph is defined first, then executed in a session.\n",
    "- **Automatic Differentiation**: TensorFlow uses `tf.GradientTape()` for automatic differentiation during backpropagation.\n",
    "- **TensorFlow Layers (Low-Level)**: These are used for fine-tuning the architecture at a granular level.\n",
    "\n",
    "#### **Model Building Steps**:\n",
    "\n",
    "##### **Step 1: Define the Model**\n",
    "Models in TensorFlow are usually defined in two main ways:\n",
    "- **Sequential API**: A linear stack of layers, ideal for simpler models.\n",
    "- **Functional API**: For more complex models with shared layers or multiple inputs and outputs.\n",
    "\n",
    "**Example using Sequential API**:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "##### **Step 2: Compile the Model**\n",
    "This involves defining the optimizer, loss function, and metrics to track during training.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "##### **Step 3: Train the Model**\n",
    "You need a dataset, which can be loaded using TensorFlow’s `tf.data` API or from existing datasets like CIFAR-10. The `model.fit()` method is used for training.\n",
    "\n",
    "```python\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "```\n",
    "\n",
    "##### **Step 4: Evaluate and Make Predictions**\n",
    "Once the model is trained, you can evaluate its performance using `model.evaluate()` and make predictions using `model.predict()`.\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(new_images)\n",
    "```\n",
    "\n",
    "#### **Internal Workings**:\n",
    "- **Forward Pass**: Inputs (tensors) are passed through each layer (like convolutional layers, activation functions, pooling, etc.).\n",
    "- **Backpropagation**: Gradients are computed automatically by `tf.GradientTape()` and used to update weights in the neural network.\n",
    "- **Optimization**: During each training step, the optimizer updates the model's weights using the computed gradients to minimize the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. PyTorch**  \n",
    "PyTorch is built around dynamic computational graphs and is highly flexible, which makes it perfect for research and experimentation.\n",
    "\n",
    "#### **Architecture Overview**:\n",
    "- **Tensors**: Just like in TensorFlow, PyTorch uses tensors as its core data structure.\n",
    "- **Dynamic Computational Graph**: Unlike TensorFlow’s static graph, PyTorch builds the graph dynamically during each forward pass.\n",
    "- **Autograd**: PyTorch uses `torch.autograd` to compute gradients automatically during backpropagation.\n",
    "- **Modules and Layers**: PyTorch models are built using `nn.Module`, where each layer or subnetwork is a module.\n",
    "\n",
    "#### **Model Building Steps**:\n",
    "\n",
    "##### **Step 1: Define the Model**\n",
    "You define models by subclassing `nn.Module` and specifying the layers and forward pass.\n",
    "\n",
    "**Example**:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(32 * 15 * 15, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 32 * 15 * 15)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "##### **Step 2: Define Loss and Optimizer**\n",
    "In PyTorch, you manually define the loss and optimizer.\n",
    "\n",
    "```python\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "##### **Step 3: Train the Model**\n",
    "You manually handle the forward pass, loss calculation, backward pass, and optimizer steps in each training epoch.\n",
    "\n",
    "```python\n",
    "for epoch in range(10):  # number of epochs\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, labels)  # Compute the loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Optimize\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1} loss: {running_loss/len(trainloader)}')\n",
    "```\n",
    "\n",
    "##### **Step 4: Evaluate and Predict**\n",
    "Evaluate using the trained model on test data.\n",
    "\n",
    "```python\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Disable gradient computation for evaluation\n",
    "    for images, labels in testloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}')\n",
    "```\n",
    "\n",
    "#### **Internal Workings**:\n",
    "- **Dynamic Graph**: The computation graph is built dynamically as the forward pass executes, which allows for flexible model architecture.\n",
    "- **Backpropagation**: PyTorch tracks all operations in the graph and uses `autograd` to compute gradients automatically.\n",
    "- **Weight Update**: Once gradients are computed, the optimizer steps in to adjust the model’s weights based on the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Keras (with TensorFlow backend)**  \n",
    "Keras is built on top of TensorFlow and provides a simplified, high-level API to build and train neural networks.\n",
    "\n",
    "#### **Architecture Overview**:\n",
    "- **Sequential and Functional APIs**: Keras primarily uses these two APIs for defining models.\n",
    "- **TensorFlow Backend**: Keras uses TensorFlow as the backend, so all operations are executed in TensorFlow’s graph.\n",
    "\n",
    "#### **Model Building Steps**:\n",
    "\n",
    "##### **Step 1: Define the Model**\n",
    "You can define models in Keras using either Sequential (simpler models) or Functional API (for more complex models).\n",
    "\n",
    "**Example using Sequential API**:\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "```\n",
    "\n",
    "##### **Step 2: Compile the Model**\n",
    "Keras abstracts the compile process, and you can specify the optimizer, loss function, and metrics.\n",
    "\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "##### **Step 3: Train the Model**\n",
    "Training is done using the `fit()` method, similar to TensorFlow.\n",
    "\n",
    "```python\n",
    "history = model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "```\n",
    "\n",
    "##### **Step 4: Evaluate and Predict**\n",
    "After training, you can evaluate and predict using Keras methods.\n",
    "\n",
    "```python\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "predictions = model.predict(new_images)\n",
    "```\n",
    "\n",
    "#### **Internal Workings**:\n",
    "- **Abstraction**: Keras simplifies many internal workings, making it easier for rapid prototyping.\n",
    "- **TensorFlow Backend**: Although Keras provides high-level APIs, it leverages TensorFlow under the hood for handling tensors, backpropagation, and optimization.\n",
    "- **Backpropagation and Optimization**: This is handled automatically by TensorFlow, and Keras focuses on simplifying the process for users.\n",
    "\n",
    "---\n",
    "\n",
    "### **Input-Output Flow in All Three Frameworks**:\n",
    "\n",
    "1. **Input**:\n",
    "   - Images are usually in the form of tensors with shape `(batch_size, height, width, channels)`.\n",
    "   - The input is passed through a series of layers like Convolutional (Conv2D), Pooling, Flatten, and Dense layers.\n",
    "   \n",
    "2. **Processing**:\n",
    "   - Each layer performs specific operations (e.g., convolution, ReLU activation, pooling) on the input data.\n",
    "   - The intermediate outputs are tensors that get passed to the next layer.\n",
    "   \n",
    "3. **Output**:\n",
    "   - The final output layer gives predictions, often as probabilities (softmax) or logits (for classification).\n",
    "   - During training, the loss between predictions and ground truth labels is computed, and gradients are used to update model weights.\n",
    "\n",
    "### **Conclusion**:\n",
    "Each framework (TensorFlow, PyTorch, and\n",
    "\n",
    " Keras) has its own approach to building, training, and deploying models, but they share similar fundamental concepts. PyTorch offers more flexibility for complex research-based projects, TensorFlow excels at scalability and production-ready applications, and Keras provides a simple interface for rapid development."
   ],
   "id": "26435893cb413e8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2a5b5c3e539598e5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
