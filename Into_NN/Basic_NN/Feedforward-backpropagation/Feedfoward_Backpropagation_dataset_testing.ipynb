{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Initialization of Weights and Biases in Neural Networks\n",
    "Setting the weights and biases of neural networks is a crucial aspect of building and training the network. Here's how weights and biases are typically initialized and updated in a neural network:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - Weights: Weights are initialized randomly at the beginning of training. The initial values of weights can significantly impact the learning process and the final performance of the network. Xavier initialization and He initialization are common techniques used to initialize weights, ensuring that they are neither too large nor too small.\n",
    "   - Biases: Biases are often initialized to small constant values, such as zeros or small random values. Unlike weights, biases are usually less sensitive to initialization, so simple initialization methods are often sufficient.\n",
    "\n",
    "2. **Forward Pass:**\n",
    "   - During the forward pass, the inputs are multiplied by the weights and added with the biases at each layer.\n",
    "   - For a given layer \\( l \\), the output \\( Z^{[l]} \\) is calculated as:\n",
    "     \\[ Z^{[l]} = W^{[l]} \\cdot A^{[l-1]} + b^{[l]} \\]\n",
    "   - Where \\( W^{[l]} \\) represents the weights matrix, \\( A^{[l-1]} \\) represents the activation of the previous layer, and \\( b^{[l]} \\) represents the bias vector of layer \\( l \\).\n",
    "\n",
    "3. **Activation Function:**\n",
    "   - After computing the linear combination of inputs, weights, and biases, the result is passed through an activation function to introduce non-linearity into the network. Common activation functions include ReLU, sigmoid, and tanh.\n",
    "\n",
    "4. **Backpropagation:**\n",
    "   - During backpropagation, the gradients of the loss function with respect to the weights and biases are computed.\n",
    "   - These gradients are then used to update the weights and biases in the opposite direction of the gradient to minimize the loss function.\n",
    "\n",
    "5. **Weight Update:**\n",
    "   - The weights are updated using an optimization algorithm such as Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "   - The update rule for the weights at each iteration is typically of the form:\n",
    "     \\[ W^{[l]} = W^{[l]} - \\alpha \\cdot \\text{d}W^{[l]} \\]\n",
    "     Where \\( \\alpha \\) is the learning rate, and \\( \\text{d}W^{[l]} \\) is the gradient of the loss function with respect to the weights.\n",
    "\n",
    "6. **Bias Update:**\n",
    "   - Biases are updated in a similar manner to weights, using the same optimization algorithm and update rule, but with gradients calculated for the biases instead.\n",
    "\n",
    "In summary, weights and biases are initialized randomly at the beginning of training, and then updated iteratively during training using backpropagation and optimization algorithms to minimize the loss function. These updates gradually improve the performance of the network on the training data."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfece933b5cf7d51"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:16:53.450380300Z",
     "start_time": "2024-03-11T00:16:53.398077500Z"
    }
   },
   "id": "initial_id",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load the training data, skipping the first row\n",
    "training_data = np.loadtxt('mnist_dataset/mnist_train.csv', delimiter=',', skiprows=1, dtype=np.float32)\n",
    "\n",
    "# Load the test data, skipping the first row\n",
    "test_data = np.loadtxt('mnist_dataset/mnist_test.csv', delimiter=',', skiprows=1, dtype=np.float32)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:00.208903500Z",
     "start_time": "2024-03-11T00:16:53.416202800Z"
    }
   },
   "id": "ac4d8f90e4d15cf7",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.shape =  (60000, 785)  ,  test_data.shape =  (10000, 785)\n"
     ]
    }
   ],
   "source": [
    "print(\"training_data.shape = \", training_data.shape, \" ,  test_data.shape = \", test_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:00.211137300Z",
     "start_time": "2024-03-11T00:17:00.204002200Z"
    }
   },
   "id": "75891bd9378b2202",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # Weight Initialization with Xavier/He : W2\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # Weight Initialization Xavier/He : W3\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "                        \n",
    "        # Initialization A3,Z3 : A3 is the result of sigmoid function about Z2\n",
    "        self.Z3 = np.zeros([1,output_nodes])\n",
    "        self.A3 = np.zeros([1,output_nodes])\n",
    "        \n",
    "        # Initialization A2,Z2\n",
    "        self.Z2 = np.zeros([1,hidden_nodes])\n",
    "        self.A2 = np.zeros([1,hidden_nodes])\n",
    "        \n",
    "        # Initialization A1,Z1\n",
    "        self.Z1 = np.zeros([1,input_nodes])    \n",
    "        self.A1 = np.zeros([1,input_nodes])       \n",
    "        \n",
    "        # Learning rate Initialization\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def feed_forward(self):  \n",
    "        \n",
    "        delta = 1e-7    # log Infinite Divergence Prevention\n",
    "        \n",
    "        # Calculate Z1,A1 in the input layer\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # Calculate Z2,A2 in the hidden layer   \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # Calculate Z3,A3 in the ouput layer\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "        \n",
    "        # Calculate the loss function value (error) : cross entropy\n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )    \n",
    "    \n",
    "    # For external printing\n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log Infinite Divergence Prevention\n",
    "        \n",
    "        # Calculate Z1,A1 in the input layer\n",
    "        self.Z1 = self.input_data\n",
    "        self.A1 = self.input_data\n",
    "        \n",
    "        # Calculate Z2,A2 in the hidden layer   \n",
    "        self.Z2 = np.dot(self.A1, self.W2) + self.b2\n",
    "        self.A2 = sigmoid(self.Z2)\n",
    "        \n",
    "        # Calculate Z3,A3 in the ouput layer\n",
    "        self.Z3 = np.dot(self.A2, self.W3) + self.b3\n",
    "        self.A3 = sigmoid(self.Z3)\n",
    "\n",
    "        # Calculate the loss function value : cross entropy\n",
    "        return  -np.sum( self.target_data*np.log(self.A3 + delta) + (1-self.target_data)*np.log((1 - self.A3)+delta ) )\n",
    "    \n",
    "    def train(self, input_data, target_data):   # input_data : 784 , target_data : 10\n",
    "        \n",
    "        self.target_data = target_data    \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        # Calculate an error with the feed foward\n",
    "        loss_val = self.feed_forward()\n",
    "        \n",
    "        # Calculate loss_3\n",
    "        loss_3 = (self.A3-self.target_data) * self.A3 * (1-self.A3)\n",
    "                        \n",
    "        # Update W3, b3 \n",
    "        self.W3 = self.W3 - self.learning_rate * np.dot(self.A2.T, loss_3)   \n",
    "        \n",
    "        self.b3 = self.b3 - self.learning_rate * loss_3\n",
    "        \n",
    "        # Caculate loss_2 \n",
    "        loss_2 = np.dot(loss_3, self.W3.T) * self.A2 * (1-self.A2)\n",
    "        \n",
    "        # Update W2, b2\n",
    "        self.W2 = self.W2 - self.learning_rate * np.dot(self.A1.T, loss_2)   \n",
    "        \n",
    "        self.b2 = self.b2 - self.learning_rate * loss_2\n",
    "  \n",
    "    def predict(self, input_data):        # Shape of input_data is (1, 784) matrix    \n",
    "        \n",
    "        Z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        A2 = sigmoid(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2, self.W3) + self.b3\n",
    "        A3 = sigmoid(Z3)\n",
    "        \n",
    "        predicted_num = np.argmax(A3)\n",
    "    \n",
    "        return predicted_num\n",
    "\n",
    "    # Accuracy measurement\n",
    "    def accuracy(self, test_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        for index in range(len(test_data)):\n",
    "                        \n",
    "            label = int(test_data[index, 0])\n",
    "                        \n",
    "            # Data normalize for one-hot encoding\n",
    "            data = (test_data[index, 1:] / 255.0 * 0.99) + 0.01\n",
    "            \n",
    "                  \n",
    "            # Vector -> Matrix (for the prediction)\n",
    "            predicted_num = self.predict(np.array(data, ndmin=2)) \n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "        print(\"Current Accuracy = \", 100*(len(matched_list)/(len(test_data))), \" %\")\n",
    "        \n",
    "        return matched_list, not_matched_list    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:00.272043600Z",
     "start_time": "2024-03-11T00:17:00.222825300Z"
    }
   },
   "id": "7166d4616ca27b96",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 ,  loss_val =  3.859061828041105\n",
      "step =  400 ,  loss_val =  1.723945632577408\n",
      "step =  800 ,  loss_val =  1.1928549870450125\n",
      "step =  1200 ,  loss_val =  0.7344099337903831\n",
      "step =  1600 ,  loss_val =  1.1983662569064677\n",
      "step =  2000 ,  loss_val =  1.7236779494244436\n",
      "step =  2400 ,  loss_val =  0.7200644094708929\n",
      "step =  2800 ,  loss_val =  0.878201592652564\n",
      "step =  3200 ,  loss_val =  0.7319273532757054\n",
      "step =  3600 ,  loss_val =  0.6821124862118942\n",
      "step =  4000 ,  loss_val =  0.9132001443741815\n",
      "step =  4400 ,  loss_val =  0.7681665597132775\n",
      "step =  4800 ,  loss_val =  0.7991229218871426\n",
      "step =  5200 ,  loss_val =  0.7818208283113939\n",
      "step =  5600 ,  loss_val =  2.383541489331239\n",
      "step =  6000 ,  loss_val =  0.8553421744938714\n",
      "step =  6400 ,  loss_val =  0.9048363276186253\n",
      "step =  6800 ,  loss_val =  0.922894875822513\n",
      "step =  7200 ,  loss_val =  0.7983763436268783\n",
      "step =  7600 ,  loss_val =  0.8765856048991398\n",
      "step =  8000 ,  loss_val =  0.9591600869283143\n",
      "step =  8400 ,  loss_val =  0.8145352077694172\n",
      "step =  8800 ,  loss_val =  0.8697965406888072\n",
      "step =  9200 ,  loss_val =  0.9012839103641298\n",
      "step =  9600 ,  loss_val =  0.8660904328412669\n",
      "step =  10000 ,  loss_val =  0.9580151681063708\n",
      "step =  10400 ,  loss_val =  0.8358347127122452\n",
      "step =  10800 ,  loss_val =  4.1869522059787085\n",
      "step =  11200 ,  loss_val =  0.9000186911167214\n",
      "step =  11600 ,  loss_val =  10.979158671566681\n",
      "step =  12000 ,  loss_val =  0.9149675477151639\n",
      "step =  12400 ,  loss_val =  0.7987751549116007\n",
      "step =  12800 ,  loss_val =  0.8955700340391177\n",
      "step =  13200 ,  loss_val =  0.9644655301534076\n",
      "step =  13600 ,  loss_val =  1.0105328392441653\n",
      "step =  14000 ,  loss_val =  0.9331008809606265\n",
      "step =  14400 ,  loss_val =  0.930929421335059\n",
      "step =  14800 ,  loss_val =  0.9548416805251143\n",
      "step =  15200 ,  loss_val =  1.0426645631871603\n",
      "step =  15600 ,  loss_val =  1.041484734480234\n",
      "step =  16000 ,  loss_val =  1.0156478433423561\n",
      "step =  16400 ,  loss_val =  0.8918777251639524\n",
      "step =  16800 ,  loss_val =  1.0664217895446655\n",
      "step =  17200 ,  loss_val =  1.0121756588788375\n",
      "step =  17600 ,  loss_val =  1.0360389602728612\n",
      "step =  18000 ,  loss_val =  1.001839841855727\n",
      "step =  18400 ,  loss_val =  0.8762950912850258\n",
      "step =  18800 ,  loss_val =  0.9086084022254917\n",
      "step =  19200 ,  loss_val =  1.012603421673726\n",
      "step =  19600 ,  loss_val =  0.9236100195870416\n",
      "step =  20000 ,  loss_val =  0.9379977392072748\n",
      "step =  20400 ,  loss_val =  1.006754685238906\n",
      "step =  20800 ,  loss_val =  1.0507831903592781\n",
      "step =  21200 ,  loss_val =  0.9142869617601582\n",
      "step =  21600 ,  loss_val =  1.0080999964689545\n",
      "step =  22000 ,  loss_val =  1.0679286102772572\n",
      "step =  22400 ,  loss_val =  0.9050872695077943\n",
      "step =  22800 ,  loss_val =  1.0154908469091888\n",
      "step =  23200 ,  loss_val =  0.9876255996003925\n",
      "step =  23600 ,  loss_val =  1.0134626945319158\n",
      "step =  24000 ,  loss_val =  1.0560182223485157\n",
      "step =  24400 ,  loss_val =  1.0259949577325929\n",
      "step =  24800 ,  loss_val =  0.9429196591465657\n",
      "step =  25200 ,  loss_val =  1.0339315090744408\n",
      "step =  25600 ,  loss_val =  0.9744954188821526\n",
      "step =  26000 ,  loss_val =  1.0076199252255198\n",
      "step =  26400 ,  loss_val =  1.020975667690207\n",
      "step =  26800 ,  loss_val =  1.1291794979390477\n",
      "step =  27200 ,  loss_val =  1.0659000495547908\n",
      "step =  27600 ,  loss_val =  1.1213233729756353\n",
      "step =  28000 ,  loss_val =  0.9691951006173787\n",
      "step =  28400 ,  loss_val =  1.110152664749772\n",
      "step =  28800 ,  loss_val =  0.9572231283612676\n",
      "step =  29200 ,  loss_val =  1.02533840894021\n",
      "step =  29600 ,  loss_val =  1.0265640784053498\n",
      "step =  30000 ,  loss_val =  1.157803264733362\n",
      "step =  30400 ,  loss_val =  0.9561066957555369\n",
      "step =  30800 ,  loss_val =  1.1013026438576072\n",
      "step =  31200 ,  loss_val =  10.415828190319402\n",
      "step =  31600 ,  loss_val =  5.586611760562489\n",
      "step =  32000 ,  loss_val =  1.0226649106609607\n",
      "step =  32400 ,  loss_val =  1.012630070981547\n",
      "step =  32800 ,  loss_val =  1.0409256854081546\n",
      "step =  33200 ,  loss_val =  1.031544786995367\n",
      "step =  33600 ,  loss_val =  0.9677562956977245\n",
      "step =  34000 ,  loss_val =  1.0604650242042293\n",
      "step =  34400 ,  loss_val =  1.1224178289609164\n",
      "step =  34800 ,  loss_val =  10.350803154028204\n",
      "step =  35200 ,  loss_val =  1.0970212468922285\n",
      "step =  35600 ,  loss_val =  0.9590920789990538\n",
      "step =  36000 ,  loss_val =  0.9628051447014934\n",
      "step =  36400 ,  loss_val =  1.0905799165213068\n",
      "step =  36800 ,  loss_val =  1.0511699955515172\n",
      "step =  37200 ,  loss_val =  1.0460937998459428\n",
      "step =  37600 ,  loss_val =  1.1378504572262709\n",
      "step =  38000 ,  loss_val =  0.9997120924490327\n",
      "step =  38400 ,  loss_val =  1.1757585015645362\n",
      "step =  38800 ,  loss_val =  1.0840809904407016\n",
      "step =  39200 ,  loss_val =  1.0884756581049861\n",
      "step =  39600 ,  loss_val =  6.755957576961689\n",
      "step =  40000 ,  loss_val =  1.1962148737967437\n",
      "step =  40400 ,  loss_val =  1.2517451805679962\n",
      "step =  40800 ,  loss_val =  1.0239844697057647\n",
      "step =  41200 ,  loss_val =  1.0462640249189206\n",
      "step =  41600 ,  loss_val =  1.0743650611664548\n",
      "step =  42000 ,  loss_val =  1.032465008147194\n",
      "step =  42400 ,  loss_val =  1.0585013017662601\n",
      "step =  42800 ,  loss_val =  1.107553733193936\n",
      "step =  43200 ,  loss_val =  1.0979049038350626\n",
      "step =  43600 ,  loss_val =  0.9238113208638273\n",
      "step =  44000 ,  loss_val =  1.1802817278082578\n",
      "step =  44400 ,  loss_val =  1.051593362889219\n",
      "step =  44800 ,  loss_val =  1.0092302828307214\n",
      "step =  45200 ,  loss_val =  1.2243042989630362\n",
      "step =  45600 ,  loss_val =  1.0178311215714886\n",
      "step =  46000 ,  loss_val =  1.0347755171648119\n",
      "step =  46400 ,  loss_val =  1.1298744878835678\n",
      "step =  46800 ,  loss_val =  1.0574779135864087\n",
      "step =  47200 ,  loss_val =  1.141070561574627\n",
      "step =  47600 ,  loss_val =  11.622668567677682\n",
      "step =  48000 ,  loss_val =  1.06018823988497\n",
      "step =  48400 ,  loss_val =  1.1319988248738708\n",
      "step =  48800 ,  loss_val =  0.9998214570630584\n",
      "step =  49200 ,  loss_val =  1.1206869837178923\n",
      "step =  49600 ,  loss_val =  1.050100375874202\n",
      "step =  50000 ,  loss_val =  1.074999713304969\n",
      "step =  50400 ,  loss_val =  0.9799026162853364\n",
      "step =  50800 ,  loss_val =  1.1239925096226022\n",
      "step =  51200 ,  loss_val =  4.452994624577437\n",
      "step =  51600 ,  loss_val =  4.204979973663931\n",
      "step =  52000 ,  loss_val =  1.0969529270850955\n",
      "step =  52400 ,  loss_val =  1.1560732274343912\n",
      "step =  52800 ,  loss_val =  6.461168501106359\n",
      "step =  53200 ,  loss_val =  1.115676839695931\n",
      "step =  53600 ,  loss_val =  1.0626364715894367\n",
      "step =  54000 ,  loss_val =  1.1409812466915596\n",
      "step =  54400 ,  loss_val =  1.0639680622221541\n",
      "step =  54800 ,  loss_val =  1.211100706871752\n",
      "step =  55200 ,  loss_val =  1.0278111092072149\n",
      "step =  55600 ,  loss_val =  0.9754951790894841\n",
      "step =  56000 ,  loss_val =  1.0460600461756153\n",
      "step =  56400 ,  loss_val =  1.0791876490921448\n",
      "step =  56800 ,  loss_val =  15.3322313705833\n",
      "step =  57200 ,  loss_val =  1.09556217169891\n",
      "step =  57600 ,  loss_val =  1.2063443039724793\n",
      "step =  58000 ,  loss_val =  1.1398084666923851\n",
      "step =  58400 ,  loss_val =  0.98233589504571\n",
      "step =  58800 ,  loss_val =  1.16601223113708\n",
      "step =  59200 ,  loss_val =  1.076570878506294\n",
      "step =  59600 ,  loss_val =  1.1626544610575724\n"
     ]
    }
   ],
   "source": [
    "# Define variables\n",
    "input_nodes = 784\n",
    "hidden_nodes = 100\n",
    "output_nodes = 10\n",
    "learning_rate = 0.3\n",
    "epochs = 1\n",
    "\n",
    "nn = NeuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    for step in range(len(training_data)):  # train\n",
    "    \n",
    "        # input_data, target_data normalize        \n",
    "        target_data = np.zeros(output_nodes) + 0.01    \n",
    "        target_data[int(training_data[step, 0])] = 0.99\n",
    "        input_data = ((training_data[step, 1:] / 255.0) * 0.99) + 0.01\n",
    "    \n",
    "        nn.train( np.array(input_data, ndmin=2), np.array(target_data, ndmin=2) )\n",
    "\n",
    "\n",
    "        # Print the error once every 400 times\n",
    "        if step % 400 == 0:\n",
    "            print(\"step = \", step,  \",  loss_val = \", nn.loss_val())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:23.806965400Z",
     "start_time": "2024-03-11T00:17:00.234425400Z"
    }
   },
   "id": "63a9ba3d4c800530",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Accuracy =  94.22  %\n"
     ]
    },
    {
     "data": {
      "text/plain": "([0,\n  1,\n  2,\n  3,\n  4,\n  5,\n  6,\n  7,\n  9,\n  10,\n  11,\n  12,\n  13,\n  14,\n  15,\n  16,\n  17,\n  18,\n  19,\n  20,\n  21,\n  22,\n  23,\n  24,\n  25,\n  26,\n  27,\n  28,\n  29,\n  30,\n  31,\n  32,\n  34,\n  35,\n  36,\n  37,\n  39,\n  40,\n  41,\n  42,\n  43,\n  44,\n  45,\n  46,\n  47,\n  48,\n  49,\n  50,\n  51,\n  52,\n  53,\n  54,\n  55,\n  56,\n  57,\n  58,\n  59,\n  60,\n  61,\n  62,\n  63,\n  64,\n  65,\n  66,\n  67,\n  68,\n  69,\n  70,\n  71,\n  72,\n  73,\n  74,\n  75,\n  76,\n  77,\n  78,\n  79,\n  80,\n  81,\n  82,\n  83,\n  84,\n  85,\n  86,\n  87,\n  88,\n  89,\n  90,\n  91,\n  92,\n  93,\n  94,\n  95,\n  96,\n  97,\n  98,\n  99,\n  100,\n  101,\n  102,\n  103,\n  104,\n  105,\n  106,\n  107,\n  108,\n  109,\n  110,\n  112,\n  113,\n  114,\n  115,\n  116,\n  117,\n  118,\n  119,\n  120,\n  121,\n  122,\n  123,\n  125,\n  126,\n  127,\n  128,\n  129,\n  130,\n  131,\n  132,\n  133,\n  134,\n  135,\n  136,\n  137,\n  138,\n  139,\n  140,\n  141,\n  142,\n  143,\n  144,\n  145,\n  146,\n  147,\n  148,\n  150,\n  152,\n  153,\n  154,\n  155,\n  156,\n  157,\n  158,\n  159,\n  160,\n  161,\n  162,\n  163,\n  164,\n  165,\n  166,\n  167,\n  168,\n  169,\n  170,\n  171,\n  172,\n  173,\n  174,\n  175,\n  176,\n  177,\n  178,\n  179,\n  180,\n  181,\n  182,\n  183,\n  184,\n  185,\n  186,\n  187,\n  188,\n  189,\n  190,\n  191,\n  192,\n  193,\n  194,\n  195,\n  196,\n  197,\n  198,\n  199,\n  200,\n  201,\n  202,\n  203,\n  204,\n  205,\n  206,\n  207,\n  208,\n  209,\n  210,\n  211,\n  212,\n  213,\n  214,\n  215,\n  216,\n  218,\n  219,\n  220,\n  221,\n  222,\n  223,\n  224,\n  225,\n  226,\n  227,\n  228,\n  229,\n  230,\n  231,\n  232,\n  233,\n  234,\n  235,\n  236,\n  237,\n  238,\n  239,\n  240,\n  242,\n  243,\n  244,\n  245,\n  246,\n  248,\n  249,\n  250,\n  251,\n  252,\n  253,\n  254,\n  255,\n  256,\n  258,\n  260,\n  261,\n  262,\n  263,\n  264,\n  265,\n  266,\n  267,\n  268,\n  269,\n  270,\n  271,\n  272,\n  273,\n  274,\n  275,\n  276,\n  277,\n  278,\n  279,\n  280,\n  281,\n  282,\n  283,\n  284,\n  285,\n  286,\n  287,\n  288,\n  289,\n  291,\n  292,\n  293,\n  294,\n  295,\n  296,\n  297,\n  298,\n  299,\n  301,\n  302,\n  303,\n  304,\n  305,\n  306,\n  307,\n  308,\n  309,\n  310,\n  311,\n  312,\n  313,\n  314,\n  315,\n  316,\n  317,\n  318,\n  319,\n  322,\n  323,\n  324,\n  325,\n  326,\n  327,\n  328,\n  329,\n  330,\n  331,\n  332,\n  333,\n  334,\n  335,\n  336,\n  337,\n  338,\n  339,\n  341,\n  342,\n  343,\n  344,\n  345,\n  346,\n  347,\n  348,\n  349,\n  350,\n  351,\n  353,\n  354,\n  355,\n  356,\n  357,\n  358,\n  359,\n  360,\n  361,\n  362,\n  363,\n  364,\n  365,\n  366,\n  367,\n  368,\n  369,\n  370,\n  371,\n  372,\n  373,\n  374,\n  375,\n  376,\n  377,\n  378,\n  379,\n  380,\n  382,\n  383,\n  384,\n  385,\n  386,\n  387,\n  388,\n  389,\n  390,\n  391,\n  392,\n  393,\n  394,\n  395,\n  396,\n  397,\n  398,\n  399,\n  400,\n  401,\n  402,\n  403,\n  404,\n  405,\n  406,\n  407,\n  408,\n  409,\n  410,\n  411,\n  412,\n  413,\n  414,\n  415,\n  416,\n  417,\n  418,\n  419,\n  420,\n  421,\n  422,\n  423,\n  424,\n  425,\n  426,\n  427,\n  428,\n  429,\n  430,\n  431,\n  432,\n  433,\n  434,\n  435,\n  436,\n  437,\n  438,\n  439,\n  440,\n  441,\n  442,\n  443,\n  446,\n  447,\n  450,\n  451,\n  452,\n  453,\n  454,\n  455,\n  456,\n  457,\n  458,\n  459,\n  460,\n  461,\n  462,\n  463,\n  464,\n  465,\n  466,\n  467,\n  468,\n  469,\n  470,\n  471,\n  472,\n  473,\n  474,\n  475,\n  476,\n  477,\n  478,\n  480,\n  481,\n  482,\n  483,\n  484,\n  485,\n  486,\n  487,\n  488,\n  489,\n  490,\n  491,\n  492,\n  493,\n  494,\n  496,\n  497,\n  498,\n  499,\n  500,\n  501,\n  502,\n  503,\n  504,\n  505,\n  506,\n  507,\n  508,\n  509,\n  510,\n  511,\n  512,\n  513,\n  514,\n  515,\n  516,\n  517,\n  518,\n  519,\n  520,\n  521,\n  522,\n  523,\n  524,\n  525,\n  526,\n  527,\n  528,\n  529,\n  530,\n  531,\n  532,\n  533,\n  534,\n  535,\n  536,\n  537,\n  538,\n  539,\n  540,\n  541,\n  542,\n  544,\n  545,\n  546,\n  547,\n  548,\n  549,\n  550,\n  552,\n  553,\n  554,\n  555,\n  556,\n  557,\n  558,\n  559,\n  560,\n  561,\n  562,\n  563,\n  564,\n  566,\n  567,\n  568,\n  569,\n  570,\n  572,\n  573,\n  574,\n  575,\n  576,\n  577,\n  579,\n  580,\n  581,\n  583,\n  584,\n  585,\n  586,\n  587,\n  588,\n  589,\n  590,\n  592,\n  593,\n  594,\n  595,\n  596,\n  597,\n  598,\n  599,\n  600,\n  601,\n  602,\n  603,\n  604,\n  605,\n  606,\n  607,\n  608,\n  609,\n  610,\n  611,\n  612,\n  614,\n  615,\n  616,\n  617,\n  618,\n  619,\n  620,\n  621,\n  622,\n  623,\n  624,\n  625,\n  626,\n  627,\n  628,\n  629,\n  630,\n  631,\n  632,\n  633,\n  634,\n  635,\n  636,\n  637,\n  638,\n  639,\n  640,\n  641,\n  642,\n  643,\n  644,\n  645,\n  646,\n  647,\n  648,\n  649,\n  650,\n  651,\n  652,\n  653,\n  654,\n  655,\n  656,\n  657,\n  658,\n  660,\n  661,\n  662,\n  663,\n  664,\n  665,\n  666,\n  668,\n  669,\n  670,\n  671,\n  672,\n  673,\n  674,\n  675,\n  676,\n  677,\n  678,\n  679,\n  680,\n  681,\n  682,\n  683,\n  684,\n  685,\n  686,\n  687,\n  688,\n  690,\n  692,\n  693,\n  694,\n  695,\n  696,\n  697,\n  698,\n  699,\n  700,\n  701,\n  702,\n  703,\n  704,\n  705,\n  706,\n  708,\n  709,\n  710,\n  711,\n  712,\n  713,\n  714,\n  715,\n  716,\n  718,\n  719,\n  721,\n  722,\n  723,\n  724,\n  725,\n  726,\n  727,\n  728,\n  729,\n  730,\n  731,\n  732,\n  733,\n  734,\n  735,\n  736,\n  737,\n  738,\n  739,\n  742,\n  743,\n  744,\n  745,\n  746,\n  747,\n  748,\n  749,\n  750,\n  751,\n  752,\n  753,\n  754,\n  755,\n  756,\n  757,\n  758,\n  759,\n  761,\n  762,\n  763,\n  764,\n  765,\n  766,\n  767,\n  768,\n  769,\n  770,\n  771,\n  772,\n  773,\n  774,\n  775,\n  776,\n  777,\n  778,\n  779,\n  780,\n  781,\n  782,\n  783,\n  784,\n  785,\n  786,\n  787,\n  788,\n  789,\n  790,\n  792,\n  793,\n  794,\n  796,\n  797,\n  798,\n  799,\n  800,\n  801,\n  802,\n  803,\n  804,\n  805,\n  806,\n  807,\n  808,\n  809,\n  811,\n  812,\n  813,\n  814,\n  815,\n  816,\n  817,\n  818,\n  819,\n  820,\n  821,\n  822,\n  823,\n  824,\n  825,\n  826,\n  827,\n  828,\n  829,\n  830,\n  831,\n  832,\n  833,\n  834,\n  835,\n  836,\n  837,\n  838,\n  839,\n  840,\n  841,\n  842,\n  843,\n  845,\n  847,\n  848,\n  849,\n  850,\n  851,\n  852,\n  853,\n  854,\n  855,\n  856,\n  857,\n  858,\n  859,\n  860,\n  861,\n  862,\n  863,\n  864,\n  865,\n  866,\n  867,\n  868,\n  869,\n  870,\n  871,\n  872,\n  873,\n  874,\n  875,\n  876,\n  877,\n  878,\n  879,\n  880,\n  882,\n  883,\n  884,\n  885,\n  886,\n  887,\n  888,\n  889,\n  890,\n  891,\n  892,\n  893,\n  894,\n  895,\n  896,\n  897,\n  899,\n  900,\n  901,\n  902,\n  903,\n  904,\n  905,\n  906,\n  907,\n  908,\n  909,\n  910,\n  911,\n  912,\n  913,\n  914,\n  915,\n  916,\n  917,\n  918,\n  919,\n  920,\n  921,\n  922,\n  923,\n  925,\n  926,\n  927,\n  928,\n  929,\n  930,\n  931,\n  932,\n  933,\n  934,\n  935,\n  936,\n  937,\n  940,\n  941,\n  942,\n  943,\n  944,\n  945,\n  946,\n  948,\n  949,\n  952,\n  953,\n  954,\n  955,\n  956,\n  957,\n  958,\n  960,\n  961,\n  962,\n  963,\n  964,\n  966,\n  967,\n  968,\n  969,\n  970,\n  971,\n  972,\n  973,\n  974,\n  975,\n  976,\n  977,\n  978,\n  979,\n  980,\n  981,\n  982,\n  983,\n  984,\n  985,\n  986,\n  987,\n  988,\n  989,\n  991,\n  992,\n  993,\n  994,\n  995,\n  996,\n  997,\n  998,\n  999,\n  1000,\n  1001,\n  1002,\n  1003,\n  1004,\n  1005,\n  1006,\n  1007,\n  1008,\n  1009,\n  1010,\n  1011,\n  1013,\n  1015,\n  1016,\n  1017,\n  1018,\n  1019,\n  1020,\n  1021,\n  1022,\n  1023,\n  1024,\n  1025,\n  1026,\n  1027,\n  1028,\n  1029,\n  1030,\n  1031,\n  1033,\n  1034,\n  1035,\n  1036,\n  1037,\n  1038,\n  1040,\n  1041,\n  1042,\n  1043,\n  1045,\n  1046,\n  1047,\n  1048,\n  1049,\n  1051,\n  1052,\n  1053,\n  1054,\n  1055,\n  1056,\n  1057,\n  1058,\n  1059,\n  1060,\n  1061,\n  1063,\n  1064,\n  1065,\n  ...],\n [8,\n  33,\n  38,\n  111,\n  124,\n  149,\n  151,\n  217,\n  241,\n  247,\n  257,\n  259,\n  290,\n  300,\n  320,\n  321,\n  340,\n  352,\n  381,\n  444,\n  445,\n  448,\n  449,\n  479,\n  495,\n  543,\n  551,\n  565,\n  571,\n  578,\n  582,\n  591,\n  613,\n  659,\n  667,\n  689,\n  691,\n  707,\n  717,\n  720,\n  740,\n  741,\n  760,\n  791,\n  795,\n  810,\n  844,\n  846,\n  881,\n  898,\n  924,\n  938,\n  939,\n  947,\n  950,\n  951,\n  959,\n  965,\n  990,\n  1012,\n  1014,\n  1032,\n  1039,\n  1044,\n  1050,\n  1062,\n  1068,\n  1073,\n  1107,\n  1112,\n  1114,\n  1119,\n  1128,\n  1131,\n  1173,\n  1181,\n  1191,\n  1192,\n  1194,\n  1198,\n  1200,\n  1204,\n  1206,\n  1226,\n  1232,\n  1242,\n  1247,\n  1252,\n  1256,\n  1260,\n  1270,\n  1283,\n  1289,\n  1299,\n  1319,\n  1326,\n  1328,\n  1337,\n  1391,\n  1393,\n  1410,\n  1433,\n  1440,\n  1446,\n  1453,\n  1466,\n  1467,\n  1469,\n  1494,\n  1500,\n  1522,\n  1525,\n  1530,\n  1549,\n  1553,\n  1581,\n  1609,\n  1634,\n  1641,\n  1678,\n  1681,\n  1696,\n  1709,\n  1716,\n  1717,\n  1718,\n  1721,\n  1722,\n  1732,\n  1737,\n  1754,\n  1772,\n  1774,\n  1790,\n  1813,\n  1828,\n  1839,\n  1868,\n  1878,\n  1899,\n  1901,\n  1911,\n  1930,\n  1938,\n  1940,\n  1941,\n  1952,\n  1963,\n  1970,\n  1984,\n  2016,\n  2024,\n  2035,\n  2040,\n  2043,\n  2044,\n  2053,\n  2063,\n  2070,\n  2098,\n  2099,\n  2109,\n  2118,\n  2121,\n  2125,\n  2129,\n  2130,\n  2135,\n  2148,\n  2174,\n  2182,\n  2185,\n  2186,\n  2189,\n  2224,\n  2266,\n  2272,\n  2293,\n  2299,\n  2325,\n  2369,\n  2371,\n  2380,\n  2387,\n  2393,\n  2394,\n  2395,\n  2406,\n  2422,\n  2425,\n  2433,\n  2447,\n  2462,\n  2488,\n  2514,\n  2526,\n  2542,\n  2573,\n  2578,\n  2598,\n  2607,\n  2610,\n  2635,\n  2648,\n  2654,\n  2670,\n  2683,\n  2695,\n  2713,\n  2730,\n  2771,\n  2780,\n  2810,\n  2877,\n  2896,\n  2905,\n  2907,\n  2919,\n  2921,\n  2925,\n  2927,\n  2939,\n  2945,\n  2952,\n  2953,\n  2995,\n  3005,\n  3060,\n  3062,\n  3073,\n  3100,\n  3108,\n  3114,\n  3117,\n  3130,\n  3136,\n  3145,\n  3157,\n  3167,\n  3189,\n  3206,\n  3225,\n  3240,\n  3269,\n  3329,\n  3333,\n  3376,\n  3384,\n  3405,\n  3410,\n  3422,\n  3436,\n  3437,\n  3490,\n  3503,\n  3520,\n  3549,\n  3558,\n  3559,\n  3565,\n  3573,\n  3575,\n  3597,\n  3604,\n  3629,\n  3681,\n  3702,\n  3716,\n  3718,\n  3726,\n  3730,\n  3751,\n  3757,\n  3767,\n  3769,\n  3776,\n  3780,\n  3794,\n  3796,\n  3801,\n  3806,\n  3808,\n  3811,\n  3817,\n  3818,\n  3820,\n  3838,\n  3848,\n  3853,\n  3855,\n  3862,\n  3876,\n  3893,\n  3902,\n  3906,\n  3926,\n  3941,\n  3943,\n  3946,\n  3951,\n  3962,\n  3968,\n  3976,\n  3995,\n  4000,\n  4007,\n  4017,\n  4027,\n  4063,\n  4065,\n  4068,\n  4075,\n  4078,\n  4093,\n  4131,\n  4140,\n  4152,\n  4163,\n  4165,\n  4176,\n  4199,\n  4201,\n  4205,\n  4211,\n  4212,\n  4224,\n  4238,\n  4248,\n  4256,\n  4271,\n  4289,\n  4297,\n  4300,\n  4306,\n  4313,\n  4315,\n  4355,\n  4374,\n  4382,\n  4433,\n  4435,\n  4449,\n  4451,\n  4477,\n  4497,\n  4498,\n  4500,\n  4523,\n  4536,\n  4540,\n  4548,\n  4567,\n  4575,\n  4578,\n  4601,\n  4615,\n  4639,\n  4671,\n  4690,\n  4695,\n  4731,\n  4740,\n  4751,\n  4785,\n  4807,\n  4808,\n  4814,\n  4823,\n  4827,\n  4837,\n  4860,\n  4863,\n  4874,\n  4876,\n  4879,\n  4880,\n  4886,\n  4888,\n  4890,\n  4896,\n  4915,\n  4939,\n  4943,\n  4950,\n  4956,\n  4966,\n  4981,\n  4990,\n  4995,\n  5046,\n  5067,\n  5068,\n  5078,\n  5140,\n  5159,\n  5246,\n  5331,\n  5440,\n  5457,\n  5569,\n  5600,\n  5617,\n  5620,\n  5623,\n  5634,\n  5642,\n  5649,\n  5714,\n  5734,\n  5735,\n  5749,\n  5835,\n  5842,\n  5858,\n  5866,\n  5887,\n  5888,\n  5891,\n  5906,\n  5913,\n  5922,\n  5926,\n  5936,\n  5937,\n  5955,\n  5972,\n  5973,\n  5975,\n  5982,\n  5985,\n  6035,\n  6042,\n  6059,\n  6065,\n  6071,\n  6081,\n  6091,\n  6109,\n  6124,\n  6157,\n  6166,\n  6172,\n  6173,\n  6347,\n  6390,\n  6391,\n  6505,\n  6511,\n  6517,\n  6555,\n  6568,\n  6574,\n  6577,\n  6597,\n  6598,\n  6599,\n  6625,\n  6632,\n  6645,\n  6651,\n  6721,\n  6744,\n  6746,\n  6765,\n  6769,\n  6785,\n  6796,\n  6926,\n  7121,\n  7130,\n  7153,\n  7216,\n  7338,\n  7391,\n  7432,\n  7434,\n  7451,\n  7539,\n  7545,\n  7637,\n  7691,\n  7797,\n  7800,\n  7821,\n  7849,\n  7851,\n  7857,\n  7858,\n  7859,\n  7876,\n  7886,\n  7902,\n  7905,\n  7915,\n  7917,\n  7918,\n  7945,\n  7990,\n  8020,\n  8062,\n  8081,\n  8091,\n  8094,\n  8095,\n  8196,\n  8246,\n  8272,\n  8277,\n  8292,\n  8339,\n  8353,\n  8406,\n  8408,\n  8410,\n  8453,\n  8520,\n  8522,\n  8952,\n  9009,\n  9010,\n  9013,\n  9015,\n  9019,\n  9024,\n  9031,\n  9036,\n  9045,\n  9209,\n  9211,\n  9280,\n  9446,\n  9482,\n  9490,\n  9513,\n  9534,\n  9538,\n  9587,\n  9595,\n  9624,\n  9634,\n  9642,\n  9664,\n  9679,\n  9698,\n  9700,\n  9716,\n  9719,\n  9726,\n  9729,\n  9744,\n  9745,\n  9749,\n  9751,\n  9752,\n  9768,\n  9770,\n  9777,\n  9779,\n  9780,\n  9808,\n  9811,\n  9832,\n  9839,\n  9856,\n  9858,\n  9867,\n  9883,\n  9888,\n  9893,\n  9905,\n  9925,\n  9941,\n  9943,\n  9944,\n  9970,\n  9975,\n  9980,\n  9982])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.accuracy(test_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:25.398872700Z",
     "start_time": "2024-03-11T00:17:23.809092700Z"
    }
   },
   "id": "a41555d34da92e96",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code implements a simple neural network for classification tasks using the MNIST dataset. Let's break down the code and explain its components along with some theory:\n",
    "\n",
    "### NeuralNetwork Class:\n",
    "- **Initialization (`__init__`):** \n",
    "  - Initializes the neural network with parameters like the number of input, hidden, and output nodes, as well as the learning rate.\n",
    "  - Initializes weights (`W2` and `W3`) using Xavier/He initialization, biases (`b2` and `b3`), and activation values (`A1`, `A2`, `A3`) to zeros.\n",
    "  \n",
    "- **Feed Forward (`feed_forward`):**\n",
    "  - Computes forward propagation through the network, calculating the activation values (`A1`, `A2`, `A3`) in each layer.\n",
    "  - Uses sigmoid activation function for hidden layers and output layer.\n",
    "  - Computes and returns the loss function value using cross-entropy loss.\n",
    "  \n",
    "- **Training (`train`):**\n",
    "  - Accepts input data and target data.\n",
    "  - Calls `feed_forward` to compute forward pass and loss.\n",
    "  - Computes loss gradients (`loss_3` and `loss_2`).\n",
    "  - Updates weights and biases (`W2`, `b2`, `W3`, `b3`) using gradient descent.\n",
    "  \n",
    "- **Prediction (`predict`):**\n",
    "  - Accepts input data.\n",
    "  - Computes forward pass to get the predicted output.\n",
    "  - Returns the predicted label (digit).\n",
    "  \n",
    "- **Accuracy (`accuracy`):**\n",
    "  - Evaluates the accuracy of the model on test data.\n",
    "  - Compares predicted labels with actual labels and calculates accuracy.\n",
    "\n",
    "### Training Loop:\n",
    "- Initializes a neural network object (`nn`) with specified parameters.\n",
    "- Iterates over the training dataset for a certain number of epochs.\n",
    "- For each training step:\n",
    "  - Normalizes input data and sets the target data for the current example.\n",
    "  - Calls the `train` method to update the weights based on the current example.\n",
    "  - Prints the loss value every 400 steps.\n",
    "\n",
    "### Explanation:\n",
    "- **Step Size (Printing Loss Every 400 Steps):**\n",
    "  - The step size of 400 is arbitrary and can be adjusted based on the user's preference or the specific needs of the training process.\n",
    "  - Printing the loss value every few steps (e.g., every 400 steps) helps monitor the training process and check if the loss is decreasing as expected.\n",
    "  - Choosing a too frequent step size can lead to excessive output, while a too large step size might not provide enough visibility into the training progress. A balance needs to be struck between the two.\n",
    "\n",
    "In summary, this code implements a basic neural network model for classifying handwritten digits from the MNIST dataset. It demonstrates key concepts such as feedforward, backpropagation, gradient descent, and Xavier/He initialization for weight initialization. The training loop iterates through the dataset, updating weights based on the calculated gradients, and periodically prints the loss value to monitor training progress."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8a40ca97c7ce6c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Xavier\n",
    " initialization, also known as Glorot initialization, is a method used to initialize the weights of neural networks. It is named after its proposer, Xavier Glorot. Xavier initialization aims to ensure that the weights are initialized in such a way that they neither explode (i.e., become too large) nor vanish (i.e., become too small) during the training process. It is particularly useful for activation functions like tanh and sigmoid.\n",
    "\n",
    "The basic idea behind Xavier initialization is to scale the initial weights according to the number of input and output units of the layer. The scale factor is inversely proportional to the square root of the number of input units. This helps in maintaining the variance of the activations and gradients approximately constant across different layers, which aids in the convergence of the training process.\n",
    "\n",
    "Mathematically, for a weight matrix \\( W \\) with dimensions \\( n_{\\text{in}} \\times n_{\\text{out}} \\), Xavier initialization initializes the weights as:\n",
    "\n",
    "\\[ W \\sim U\\left(-\\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{n_{\\text{in}} + n_{\\text{out}}}}\\right) \\]\n",
    "\n",
    "Where \\( U(a, b) \\) denotes a uniform distribution in the interval \\( [a, b] \\).\n",
    "\n",
    "Now, let's discuss how weights are adjusted during the training process:\n",
    "\n",
    "1. **Forward Pass (Feedforward):** During the forward pass, input data is propagated through the network, and predictions are made. At each layer, the input is multiplied by the weight matrix and passed through an activation function to produce the output.\n",
    "\n",
    "2. **Loss Calculation:** After the forward pass, the loss between the predicted output and the actual target is calculated. This loss function quantifies how far the predictions are from the actual targets.\n",
    "\n",
    "3. **Backpropagation:** Backpropagation is the process of computing gradients of the loss function with respect to the weights of the network. It works by recursively applying the chain rule of calculus from the output layer to the input layer.\n",
    "\n",
    "4. **Weight Update:** Once the gradients are computed, the weights are updated to minimize the loss function. This is typically done using an optimization algorithm such as Stochastic Gradient Descent (SGD) or its variants. The weights are adjusted in the opposite direction of the gradient, scaled by a learning rate, which controls the size of the update step.\n",
    "\n",
    "Regarding the value of 400 for the step size in your code, it seems to be used as a parameter for printing the loss value during training. Printing the loss value every 400 steps might be chosen to monitor the progress of training without overwhelming the console with too many print statements. This value can be adjusted based on factors such as the size of the dataset, the complexity of the model, and the computational resources available."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39f4847de1f44ff2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T00:17:25.398872700Z",
     "start_time": "2024-03-11T00:17:25.391248800Z"
    }
   },
   "id": "33ffdea5c4966bb6",
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
